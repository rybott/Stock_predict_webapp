{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Neural Network utilizing Tweet Data as well as Stock/SP500 data\n",
    "This model, from my testing, could not surpass the Random Forest Model as being more accurate, and due lack of twitter data, this model was never used in the final program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = pd.read_excel(\"Data_all082323.xlsx\")\n",
    "data_clean = data.drop([\"date\",\"sp_rf\"], axis=1)\n",
    "print(data_clean.info())\n",
    "\n",
    "# Check Balance\n",
    "print(data_clean['Predict'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "epoch_no = 100\n",
    "lr = .0001\n",
    "\n",
    "train_data, test_data = train_test_split(data_clean, test_size=0.2)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_train = train_data.drop('Predict', axis=1)\n",
    "X_train_col = X_train.columns\n",
    "X_train = X_train.astype('float32')\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=X_train_col)\n",
    "y_train = train_data['Predict']\n",
    "\n",
    "\n",
    "X_test = test_data.drop('Predict', axis=1)\n",
    "X_test_col = X_test.columns\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test, columns=X_test_col)\n",
    "y_test = test_data['Predict']\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).shuffle(len(train_data)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    layers.Dense(32, activation='relu'),# Add more layers here\n",
    "    layers.Dense(1, activation='sigmoid')  # sigmoid for binary classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compling\n",
    "adam_optimizer = optimizers.Adam(learning_rate=lr)\n",
    "#adam_optimizer = 'adam'\n",
    "\n",
    "model.compile(optimizer=adam_optimizer,\n",
    "              loss='binary_crossentropy',  # appropriate for binary classification\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(train_dataset, epochs=epoch_no, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining Accruacy\n",
    "\n",
    "# Extract accuracy values\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Create epochs range\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'NN_stockuniq_model'\n",
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
